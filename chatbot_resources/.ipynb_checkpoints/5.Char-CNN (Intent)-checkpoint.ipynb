{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load done\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib.image import imread, imsave \n",
    "import matplotlib.pyplot as plt  \n",
    "import pandas as pd\n",
    "from konlpy.tag import Mecab\n",
    "from gensim.models import word2vec\n",
    "print(\"load done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['판교에 오늘 피자 주문해줘', '오늘 날짜에 호텔 예약 해줄레', '모래 날짜에 판교 여행 정보 알려줘']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_size = 50\n",
    "encode_length = 4\n",
    "label_size = 3\n",
    "embed_type = \"onehot\" #onehot or w2v\n",
    "# Choose single test\n",
    "# filter_type = \"single\"\n",
    "# filter_number = 32\n",
    "# filter_size = 2\n",
    "\n",
    "# Choose multi test\n",
    "filter_type = \"multi\"\n",
    "filter_sizes = [2,3,4,2,3,4,2,3,4]\n",
    "num_filters = len(filter_sizes)\n",
    "\n",
    "train_data_list =  {\n",
    "                'encode' : ['판교에 오늘 피자 주문해줘','오늘 날짜에 호텔 예약 해줄레','모래 날짜에 판교 여행 정보 알려줘'],\n",
    "                'decode' : ['0','1','2']\n",
    "             }\n",
    "train_data_list.get('encode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['판교에 오늘 피자 주문해줘', '오늘 날짜에 호텔 예약 해줄레', '모래 날짜에 판교 여행 정보 알려줘']\n",
      "Word2Vec(vocab=15, size=50, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "def train_vector_model(str_buf):\n",
    "\n",
    "    mecab = Mecab('/usr/local/lib/mecab/dic/mecab-ko-dic')\n",
    "    str_buf = train_data_list['encode']\n",
    "    pos1 = mecab.pos(''.join(str_buf))\n",
    "    pos2 = ' '.join(list(map(lambda x : '\\n' if x[1] in ['SF'] else x[0], pos1))).split('\\n')\n",
    "    morphs = list(map(lambda x : mecab.morphs(x) , pos2))\n",
    "    print(str_buf)\n",
    "    model = word2vec.Word2Vec(morphs, size=vector_size, window=2, min_count=1, iter=100)\n",
    "#     model.build_vocab(morphs)\n",
    "#     model.train(morphs)\n",
    "    return model\n",
    "model = train_vector_model(train_data_list)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(data_path):\n",
    "    df_csv_read = pd.DataFrame(data_path)\n",
    "    return df_csv_read\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed word to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(data) : \n",
    "    mecab = Mecab('/usr/local/lib/mecab/dic/mecab-ko-dic')\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for encode_raw in data['encode'] : \n",
    "        encode_raw = mecab.morphs(encode_raw)\n",
    "        encode_raw = list(map(lambda x : encode_raw[x] if x < len(encode_raw) else '#', range(encode_length)))\n",
    "        if(embed_type == 'onehot') :\n",
    "            bucket = np.zeros(vector_size, dtype=float).copy()\n",
    "            input = np.array(list(map(lambda x : onehot_vectorize(bucket, x) if x in model.wv.index2word else np.zeros(vector_size,dtype=float) , encode_raw)))\n",
    "        else : \n",
    "            input = np.array(list(map(lambda x : model[x] if x in model.wv.index2word else np.zeros(vector_size,dtype=float) , encode_raw)))\n",
    "        inputs.append(input.flatten())\n",
    "        \n",
    "    for decode_raw in data['decode']: \n",
    "        label = np.zeros(label_size, dtype=float)\n",
    "        np.put(label, decode_raw, 1)\n",
    "        labels.append(label)\n",
    "    return inputs, labels\n",
    "\n",
    "def onehot_vectorize(bucket, x):\n",
    "    np.put(bucket, model.wv.index2word.index(x),1)\n",
    "    return bucket\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embed word to vector on predict step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_embed(data) : \n",
    "    mecab = Mecab('/usr/local/lib/mecab/dic/mecab-ko-dic')\n",
    "    encode_raw = mecab.morphs(data)\n",
    "    encode_raw = list(map(lambda x : encode_raw[x] if x < len(encode_raw) else '#', range(encode_length)))\n",
    "    if(embed_type == 'onehot') :\n",
    "        bucket = np.zeros(vector_size, dtype=float).copy()\n",
    "        input = np.array(list(map(lambda x : onehot_vectorize(bucket, x) if x in model.wv.index2word else np.zeros(vector_size,dtype=float) , encode_raw)))\n",
    "    else : \n",
    "        input = np.array(list(map(lambda x : model[x] if x in model.wv.index2word else np.zeros(vector_size,dtype=float) , encode_raw)))\n",
    "    return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get train and test data for feed on tensorflow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data():\n",
    "    train_data, train_label = embed(load_csv(train_data_list))\n",
    "    test_data, test_label = embed(load_csv(train_data_list))\n",
    "    return train_label, test_label, train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create graph with single filter size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define cnn graph func\n"
     ]
    }
   ],
   "source": [
    "def create_s_graph(train=True):\n",
    "    # placeholder is used for feeding data.\n",
    "    x = tf.placeholder(\"float\", shape=[None, encode_length * vector_size], name = 'x') \n",
    "    y_target = tf.placeholder(\"float\", shape=[None, label_size], name = 'y_target') \n",
    "\n",
    "    # reshape input data\n",
    "    x_image = tf.reshape(x, [-1,encode_length,vector_size,1], name=\"x_image\")\n",
    "    \n",
    "    # Build a convolutional layer and maxpooling with random initialization\n",
    "    W_conv1 = tf.Variable(tf.truncated_normal([filter_size, filter_size, 1, filter_number], stddev=0.1), name=\"W_conv1\") # W is [row, col, channel, feature]\n",
    "    b_conv1 = tf.Variable(tf.zeros([filter_number]), name=\"b_conv1\")\n",
    "    h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1, name=\"h_conv1\")\n",
    "    h_pool1 = tf.nn.max_pool( h_conv1 , ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name = \"h_pool1\")\n",
    "    \n",
    "    # Build a fully connected layer\n",
    "    h_pool2_flat = tf.reshape(h_pool1, [-1, int((encode_length/2)*(vector_size/2))*filter_number], name=\"h_pool2_flat\")\n",
    "    W_fc1 = tf.Variable(tf.truncated_normal([int((encode_length/2)*(vector_size/2))*filter_number, 256], stddev=0.1), name = 'W_fc1')\n",
    "    b_fc1 = tf.Variable(tf.zeros([256]), name = 'b_fc1')\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1, name=\"h_fc1\")\n",
    "    \n",
    "    keep_prob = 1.0\n",
    "    if(train) : \n",
    "        # Dropout Layer\n",
    "        keep_prob = tf.placeholder(\"float\", name=\"keep_prob\")\n",
    "        h_fc1 = tf.nn.dropout(h_fc1, keep_prob, name=\"h_fc1_drop\")\n",
    "    \n",
    "    # Build a fully connected layer with softmax \n",
    "    W_fc2 = tf.Variable(tf.truncated_normal([256, label_size], stddev=0.1), name = 'W_fc2')\n",
    "    b_fc2 = tf.Variable(tf.zeros([label_size]), name = 'b_fc2')\n",
    "    #y=tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2, name=\"y\")\n",
    "    y=tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "    \n",
    "    # define the Loss function\n",
    "    #cross_entropy = -tf.reduce_sum(y_target*tf.log(y), name = 'cross_entropy')\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_target))\n",
    "    \n",
    "    # define optimization algorithm\n",
    "    #train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_target, 1))\n",
    "    # correct_prediction is list of boolean which is the result of comparing(model prediction , data)\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) \n",
    "    # tf.cast() : changes true -> 1 / false -> 0\n",
    "    # tf.reduce_mean() : calculate the mean\n",
    "    \n",
    "    return accuracy, x, y_target, keep_prob, train_step, y, cross_entropy, W_conv1\n",
    "    \n",
    "print(\"define cnn graph func\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create graph with multi filter size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "define cnn graph func\n"
     ]
    }
   ],
   "source": [
    "def create_m_graph(train=True):\n",
    "    # placeholder is used for feeding data.\n",
    "    x = tf.placeholder(\"float\", shape=[None, encode_length * vector_size], name = 'x') \n",
    "    y_target = tf.placeholder(\"float\", shape=[None, label_size], name = 'y_target') \n",
    "\n",
    "    # reshape input data\n",
    "    x_image = tf.reshape(x, [-1,encode_length,vector_size,1], name=\"x_image\")\n",
    "    # Keeping track of l2 regularization loss (optional)\n",
    "    l2_loss = tf.constant(0.0)\n",
    "    \n",
    "    pooled_outputs = []\n",
    "    for i, filter_size in enumerate(filter_sizes):\n",
    "        with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "            # Convolution Layer\n",
    "            filter_shape = [filter_size, vector_size, 1, num_filters]\n",
    "            W_conv1 = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            b_conv1 = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "            \n",
    "            conv = tf.nn.conv2d(\n",
    "                x_image,\n",
    "                W_conv1,\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding=\"VALID\",\n",
    "                name=\"conv\")\n",
    "            \n",
    "            # Apply nonlinearity\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b_conv1), name=\"relu\")\n",
    "            # Maxpooling over the outputs\n",
    "            pooled = tf.nn.max_pool(\n",
    "                h,\n",
    "                ksize=[1, encode_length - filter_size + 1, 1, 1],\n",
    "                strides=[1, 1, 1, 1],\n",
    "                padding='VALID',\n",
    "                name=\"pool\")\n",
    "            pooled_outputs.append(pooled)\n",
    "\n",
    "    # Combine all the pooled features\n",
    "    num_filters_total = num_filters * len(filter_sizes)\n",
    "    h_pool = tf.concat(pooled_outputs, 3)\n",
    "    h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    " \n",
    "    # Add dropout\n",
    "    keep_prob = 1.0\n",
    "    if(train) : \n",
    "        keep_prob = tf.placeholder(\"float\", name=\"keep_prob\")\n",
    "        h_pool_flat = tf.nn.dropout(h_pool_flat, keep_prob)\n",
    "\n",
    "    # Final (unnormalized) scores and predictions\n",
    "    W_fc1 = tf.get_variable(\n",
    "        \"W_fc1\",\n",
    "        shape=[num_filters_total, label_size],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_fc1 = tf.Variable(tf.constant(0.1, shape=[label_size]), name=\"b\")\n",
    "    l2_loss += tf.nn.l2_loss(W_fc1)\n",
    "    l2_loss += tf.nn.l2_loss(b_fc1)\n",
    "    y = tf.nn.xw_plus_b(h_pool_flat, W_fc1, b_fc1, name=\"scores\")\n",
    "    predictions = tf.argmax(y, 1, name=\"predictions\")\n",
    "\n",
    "    # CalculateMean cross-entropy loss\n",
    "    losses = tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_target)\n",
    "    cross_entropy = tf.reduce_mean(losses)\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    \n",
    "    # Accuracy\n",
    "    correct_predictions = tf.equal(predictions, tf.argmax(y_target, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    \n",
    "    return accuracy, x, y_target, keep_prob, train_step, y, cross_entropy, W_conv1\n",
    "    \n",
    "print(\"define cnn graph func\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize weight matrix function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_layer(weight_list) :\n",
    "    if(filter_type == 'multi') : \n",
    "        show = np.array(weight_list).reshape(num_filters, filter_sizes[np.argmax(filter_sizes)], vector_size)\n",
    "        for i, matrix in enumerate(show) :\n",
    "            fig = plt.figure()\n",
    "            plt.imshow(matrix)\n",
    "        plt.show()\n",
    "    else : \n",
    "        show = np.array(weight_list).reshape(32, 2, 2)\n",
    "        for i, matrix in enumerate(show) :\n",
    "            fig = plt.figure()\n",
    "            plt.imshow(matrix)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-7b9853ecf6c3>:60: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-12-e2910887040c>:17: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "step 0, training accuracy: 0.000\n",
      "step 10, training accuracy: 0.333\n",
      "step 20, training accuracy: 0.333\n",
      "step 30, training accuracy: 0.667\n",
      "step 40, training accuracy: 0.667\n",
      "step 50, training accuracy: 1.000\n",
      "step 60, training accuracy: 1.000\n",
      "step 70, training accuracy: 1.000\n",
      "step 80, training accuracy: 1.000\n",
      "step 90, training accuracy: 1.000\n",
      "step 100, training accuracy: 1.000\n",
      "step 110, training accuracy: 1.000\n",
      "step 120, training accuracy: 1.000\n",
      "step 130, training accuracy: 1.000\n",
      "step 140, training accuracy: 1.000\n",
      "step 150, training accuracy: 1.000\n",
      "step 160, training accuracy: 1.000\n",
      "step 170, training accuracy: 1.000\n",
      "step 180, training accuracy: 1.000\n",
      "step 190, training accuracy: 1.000\n",
      "test accuracy: 1\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "def run() : \n",
    "    try : \n",
    "        # get Data \n",
    "        labels_train, labels_test, data_filter_train, data_filter_test = get_test_data()\n",
    "        # reset Graph\n",
    "        tf.reset_default_graph()   \n",
    " \n",
    "        # Create Session\n",
    "        sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth =True)))  \n",
    "        # create graph\n",
    "        if(filter_type == 'single') :\n",
    "            accuracy, x, y_target, keep_prob, train_step, y, cross_entropy, W_conv1 = create_s_graph(train=True)\n",
    "        else :\n",
    "            accuracy, x, y_target, keep_prob, train_step, y, cross_entropy, W_conv1 = create_m_graph(train=True)\n",
    "            \n",
    "        # set saver\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "        # initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        # training the MLP\n",
    "        for i in range(200): \n",
    "            sess.run(train_step, feed_dict={x: data_filter_train, y_target: labels_train, keep_prob: 0.5})\n",
    "            if i%10 == 0:\n",
    "                train_accuracy = sess.run(accuracy, feed_dict={x:data_filter_train, y_target: labels_train, keep_prob: 1})\n",
    "                print (\"step %d, training accuracy: %.3f\"%(i, train_accuracy))\n",
    "                \n",
    "        # for given x, y_target data set\n",
    "        print  (\"test accuracy: %g\"% sess.run(accuracy, feed_dict={x:data_filter_test, y_target: labels_test, keep_prob: 1}))\n",
    "        \n",
    "        # show weight matrix as image \n",
    "        weight_vectors = sess.run(W_conv1, feed_dict={x: data_filter_train, y_target: labels_train, keep_prob: 1.0})\n",
    "        #show_layer(weight_vectors)\n",
    "        \n",
    "        # Save Model\n",
    "        path = './model/'\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "            print(\"path created\")\n",
    "        saver.save(sess, path)\n",
    "        print(\"model saved\")\n",
    "    except Exception as e : \n",
    "        raise Exception (\"error on training: {0}\".format(e))\n",
    "    finally :\n",
    "        sess.close()\n",
    "\n",
    "# run stuff\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in dict : ['에', '판교', '오늘', '해', '날짜', '피자', '주문', '줘', '호텔', '예약', '줄레', '모래', '여행', '정보', '알려줘']\n",
      "INFO:tensorflow:Restoring parameters from ./model/\n",
      "model restored\n",
      "result : [array([[ 0.55470884, -0.23161334, -0.52453035]], dtype=float32)]\n",
      "result : 0\n",
      "INFO:tensorflow:Restoring parameters from ./model/\n",
      "model restored\n",
      "result : [array([[-0.50112414,  0.9452978 ,  0.18715186]], dtype=float32)]\n",
      "result : 1\n",
      "INFO:tensorflow:Restoring parameters from ./model/\n",
      "model restored\n",
      "result : [array([[ 0.05691623, -0.2832873 ,  0.8428901 ]], dtype=float32)]\n",
      "result : 2\n"
     ]
    }
   ],
   "source": [
    "def predict(test_data) : \n",
    "    try : \n",
    "        # reset Graph\n",
    "        tf.reset_default_graph()   \n",
    "        # Create Session\n",
    "        sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth =True)))  \n",
    "        # create graph\n",
    "        if(filter_type == 'single') :\n",
    "            _, x, _, _, _, y, _, _ = create_s_graph(train=False)\n",
    "        else : \n",
    "            _, x, _, _, _, y, _, _ = create_m_graph(train=False)\n",
    "        \n",
    "        # initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # set saver\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        # Restore Model\n",
    "        path = './model/'\n",
    "        if os.path.exists(path):\n",
    "            saver.restore(sess, path)\n",
    "            print(\"model restored\")\n",
    "\n",
    "        # training the MLP\n",
    "        #print(\"input data : {0}\".format(test_data))\n",
    "        y = sess.run([y], feed_dict={x: np.array([test_data])})\n",
    "        print(\"result : {0}\".format(y))\n",
    "        print(\"result : {0}\".format(np.argmax(y)))\n",
    "        \n",
    "    except Exception as e : \n",
    "        raise Exception (\"error on training: {0}\".format(e))\n",
    "    finally :\n",
    "        sess.close()\n",
    "\n",
    "print(\"words in dict : {0}\".format(model.wv.index2word))\n",
    "\n",
    "predict(np.array(inference_embed(\"판교에 오늘 피자 주문해줘\")).flatten())\n",
    "predict(np.array(inference_embed(\"오늘 날짜에 호텔 예약 해줄수있어\")).flatten())\n",
    "predict(np.array(inference_embed(\"모래 날짜에 판교 여행 정보 알려줘\")).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
